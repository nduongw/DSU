{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/anaconda3/envs/ndg2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/disk1/anaconda3/envs/ndg2/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /mnt/disk1/anaconda3/envs/ndg2/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import net\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from dassl.config import get_cfg_default\n",
    "from dassl.engine import build_trainer\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'ndg2 (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "do_interpolation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'ndg2 (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def print_args(args, cfg):\n",
    "    pass\n",
    "    # print('***************')\n",
    "    # print('** Arguments **')\n",
    "    # print('***************')\n",
    "    # optkeys = list(args.__dict__.keys())\n",
    "    # optkeys.sort()\n",
    "    # for key in optkeys:\n",
    "    #     print('{}: {}'.format(key, args.__dict__[key]))\n",
    "    # print('************')\n",
    "    # print('** Config **')\n",
    "    # print('************')\n",
    "    # print(cfg)\n",
    "    \n",
    "def reset_cfg(cfg, args):\n",
    "    if args.root:\n",
    "        cfg.DATASET.ROOT = args.root\n",
    "\n",
    "    if args.output_dir:\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "\n",
    "    if args.resume:\n",
    "        cfg.RESUME = args.resume\n",
    "\n",
    "    if args.seed:\n",
    "        cfg.SEED = args.seed\n",
    "\n",
    "    if args.source_domains:\n",
    "        cfg.DATASET.SOURCE_DOMAINS = args.source_domains\n",
    "\n",
    "    if args.target_domains:\n",
    "        cfg.DATASET.TARGET_DOMAINS = args.target_domains\n",
    "\n",
    "    if args.transforms:\n",
    "        cfg.INPUT.TRANSFORMS = args.transforms\n",
    "\n",
    "    if args.trainer:\n",
    "        cfg.TRAINER.NAME = args.trainer\n",
    "\n",
    "    if args.backbone:\n",
    "        cfg.MODEL.BACKBONE.NAME = args.backbone\n",
    "\n",
    "    if args.head:\n",
    "        cfg.MODEL.HEAD.NAME = args.head\n",
    "\n",
    "    #if args.uncertainty:\n",
    "    cfg.MODEL.UNCERTAINTY = args.uncertainty\n",
    "\n",
    "    #if args.pos:\n",
    "    cfg.MODEL.POS = args.pos\n",
    "    # cfg.USE_CUDA = False\n",
    "    \n",
    "def setup_cfg(args):\n",
    "    cfg = get_cfg_default()\n",
    "    reset_cfg(cfg, args)\n",
    "    if args.dataset_config_file:\n",
    "        cfg.merge_from_file(args.dataset_config_file)\n",
    "    if args.config_file:\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'ndg2 (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--root', type=str, default='./DATA', help='path to dataset')\n",
    "parser.add_argument(\n",
    "    '--output-dir', type=str, default='../output/dg/pacs/baseline/art_painting', help='output directory'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--resume',\n",
    "    type=str,\n",
    "    default='',\n",
    "    help='checkpoint directory (from which the training resumes)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help='only positive value enables a fixed seed'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--source-domains',\n",
    "    type=str,\n",
    "    nargs='+',\n",
    "    default='cartoon photo sketch'.split(),\n",
    "    help='source domains for DA/DG'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--target-domains',\n",
    "    type=str,\n",
    "    nargs='+',\n",
    "    default=['art_painting'],\n",
    "    help='target domains for DA/DG'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--transforms', type=str, nargs='+', help='data augmentation methods'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--config-file', type=str, default='../configs/trainers/dg/vanilla/pacs.yaml', help='path to config file'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--dataset-config-file',\n",
    "    type=str,\n",
    "    default='../configs/datasets/dg/pacs_total.yaml',\n",
    "    help='path to config file for dataset setup'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--trainer', type=str, default='Vanilla', help='name of trainer'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--backbone', type=str, default='', help='name of CNN backbone'\n",
    ")\n",
    "parser.add_argument('--head', type=str, default='', help='name of head')\n",
    "parser.add_argument(\n",
    "    '--eval-only', action='store_true', help='evaluation only'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--model-dir',\n",
    "    type=str,\n",
    "    default='',\n",
    "    help='load model from this directory for eval-only mode'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--load-epoch',\n",
    "    type=int,\n",
    "    help='load model weights at this epoch for evaluation'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--no-train', action='store_true', help='do not call trainer.train()'\n",
    ")\n",
    "parser.add_argument(\n",
    "    'opts',\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER,\n",
    "    help='modify config options using the command-line'\n",
    ")\n",
    "parser.add_argument('--uncertainty', default=0.0, type=float)\n",
    "parser.add_argument('--pos', nargs='+', type=int, default=[],\n",
    "                    help='pos for uncertainty')\n",
    "parser.add_argument('--wandb', default=0, type=int, help='visualize on Wandb')\n",
    "\n",
    "parser.add_argument('--content', type=str,\n",
    "                    help='File path to the content image')\n",
    "parser.add_argument('--content_dir', type=str, default='../DATA/pacs/images/sketch/person',\n",
    "                    help='Directory path to a batch of content images')\n",
    "parser.add_argument('--style', type=str, default='../DATA/pacs/images/art_painting/person/pic_011.jpg', help='File path to the style image, or multiple style images separated by commas if you want to do style interpolation or spatial control')\n",
    "parser.add_argument('--style_dir', type=str,\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--vgg', type=str, default='models/vgg_normalised.pth')\n",
    "parser.add_argument('--decoder', type=str, default='models/decoder.pth')\n",
    "\n",
    "parser.add_argument('--save_dir', default='./experiments',\n",
    "                    help='Directory to save the model')\n",
    "parser.add_argument('--store_folder', default='./styled_images/pacs/images/sketch/person',\n",
    "                    help='Directory to save the output image(s)')\n",
    "parser.add_argument('--content_size', type=int, default=0,\n",
    "                    help='New (minimum) size for the content image, \\\n",
    "                    keeping the original size if set to 0')\n",
    "parser.add_argument('--style_size', type=int, default=0,\n",
    "                    help='New (minimum) size for the style image, \\\n",
    "                    keeping the original size if set to 0')\n",
    "parser.add_argument('--crop', action='store_true',\n",
    "                    help='do center crop to create squared image')\n",
    "parser.add_argument('--save_ext', default='.jpg',\n",
    "                    help='The extension name of the output image')\n",
    "parser.add_argument('--output', type=str, default='output',\n",
    "                    help='Directory to save the output image(s)')\n",
    "\n",
    "# Advanced options\n",
    "parser.add_argument('--preserve_color', action='store_true',\n",
    "                    help='If specified, preserve color of the content image')\n",
    "parser.add_argument('--alpha', type=float, default=1.0,\n",
    "                    help='The weight that controls the degree of \\\n",
    "                             stylization. Should be between 0 and 1')\n",
    "parser.add_argument(\n",
    "    '--style_interpolation_weights', type=str, default='',\n",
    "    help='The weight for blending the style of multiple style images')\n",
    "args = parser.parse_args('')\n",
    "\n",
    "cfg = setup_cfg(args)\n",
    "if args.wandb:\n",
    "    if 'u' in cfg.MODEL.BACKBONE.NAME:\n",
    "        job_type = 'DSU'\n",
    "    elif 'c' in cfg.MODEL.BACKBONE.NAME:\n",
    "        job_type = 'ConstStyle'\n",
    "    else:\n",
    "        job_type = 'Baseline'\n",
    "    if cfg.MODEL.BACKBONE.PRETRAINED:\n",
    "        job_type += '-pretrained'\n",
    "        \n",
    "    tracker = wandb.init(\n",
    "        project = 'StyleDG',\n",
    "        entity = 'aiotlab',\n",
    "        config = args,\n",
    "        group = f'{cfg.DATASET.NAME}',\n",
    "        name = f'train={cfg.DATASET.SOURCE_DOMAINS}_test={cfg.DATASET.TARGET_DOMAINS}_type={args.option}',\n",
    "        job_type = job_type\n",
    "    )\n",
    "    args.tracker = tracker\n",
    "\n",
    "if cfg.SEED >= 0:\n",
    "    print('Setting fixed seed: {}'.format(cfg.SEED))\n",
    "    set_random_seed(cfg.SEED)\n",
    "setup_logger(cfg.OUTPUT_DIR)\n",
    "\n",
    "if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print_args(args, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'ndg2 (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output_dir = Path(args.store_folder)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (args.content or args.content_dir)\n",
    "if args.content:\n",
    "    content_paths = [Path(args.content)]\n",
    "else:\n",
    "    content_dir = Path(args.content_dir)\n",
    "    content_paths = [f for f in content_dir.glob('*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (args.style or args.style_dir)\n",
    "if args.style:\n",
    "    style_paths = args.style.split(',')\n",
    "    if len(style_paths) == 1:\n",
    "        style_paths = [Path(args.style)]\n",
    "    else:\n",
    "        do_interpolation = True\n",
    "        assert (args.style_interpolation_weights != ''), \\\n",
    "            'Please specify interpolation weights'\n",
    "        weights = [int(i) for i in args.style_interpolation_weights.split(',')]\n",
    "        interpolation_weights = [w / sum(weights) for w in weights]\n",
    "else:\n",
    "    style_dir = Path(args.style_dir)\n",
    "    style_paths = [f for f in style_dir.glob('*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer: Vanilla\n",
      "Loading dataset: TotalPACS\n",
      "Building transform_train\n",
      "+ resize to 224x224\n",
      "+ random flip\n",
      "+ random translation\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "Building transform_test\n",
      "+ resize to 224x224\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "***** Dataset statistics *****\n",
      "  Dataset: TotalPACS\n",
      "  Source domains: ['cartoon', 'photo', 'sketch']\n",
      "  Target domains: ['art_painting']\n",
      "  # classes: 7\n",
      "  # train_x: 7,136\n",
      "  # val: 806\n",
      "  # test: 2,048\n",
      "Building model\n",
      "Backbone: resnet18\n",
      "SimpleNet(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n",
      "# params: 11,180,103\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "trainer = build_trainer(cfg, args)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "decoder = net.decoder\n",
    "vgg = net.vgg\n",
    "\n",
    "decoder.eval()\n",
    "vgg.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load('experiments/decoder_iter_88000.pth.tar'))\n",
    "vgg.load_state_dict(torch.load('models/vgg_normalised.pth'))\n",
    "vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "\n",
    "vgg.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "train_loader_x = trainer.train_loader_x\n",
    "test_loader = trainer.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "def adaptive_instance_normalization(content_feat, style_feat):\n",
    "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
    "    size = content_feat.size()\n",
    "    style_mean, style_std = calc_mean_std(style_feat)\n",
    "    content_mean, content_std = calc_mean_std(content_feat)\n",
    "\n",
    "    normalized_feat = (content_feat - content_mean.expand(\n",
    "        size)) / content_std.expand(size)\n",
    "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
    "\n",
    "def test_transform(size, crop):\n",
    "    transform_list = []\n",
    "    if size != 0:\n",
    "        transform_list.append(transforms.Resize(size))\n",
    "    if crop:\n",
    "        transform_list.append(transforms.CenterCrop(size))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "\n",
    "def style_transfer(vgg, decoder, content, style, alpha=1.0,\n",
    "                   interpolation_weights=None):\n",
    "    assert (0.0 <= alpha <= 1.0)\n",
    "    content_f = vgg(content)\n",
    "    style_f = vgg(style)\n",
    "    if interpolation_weights:\n",
    "        _, C, H, W = content_f.size()\n",
    "        feat = torch.FloatTensor(1, C, H, W).zero_().to(device)\n",
    "        base_feat = adaptive_instance_normalization(content_f, style_f)\n",
    "        for i, w in enumerate(interpolation_weights):\n",
    "            feat = feat + w * base_feat[i:i + 1]\n",
    "        content_f = content_f[0:1]\n",
    "    else:\n",
    "        feat = adaptive_instance_normalization(content_f, style_f)\n",
    "    feat = feat * alpha + content_f * (1 - alpha)\n",
    "    return decoder(feat)\n",
    "\n",
    "def coral(source, target):\n",
    "    # assume both source and target are 3D array (C, H, W)\n",
    "    # Note: flatten -> f\n",
    "\n",
    "    source_f, source_f_mean, source_f_std = _calc_feat_flatten_mean_std(source)\n",
    "    source_f_norm = (source_f - source_f_mean.expand_as(\n",
    "        source_f)) / source_f_std.expand_as(source_f)\n",
    "    source_f_cov_eye = \\\n",
    "        torch.mm(source_f_norm, source_f_norm.t()) + torch.eye(3)\n",
    "\n",
    "    target_f, target_f_mean, target_f_std = _calc_feat_flatten_mean_std(target)\n",
    "    target_f_norm = (target_f - target_f_mean.expand_as(\n",
    "        target_f)) / target_f_std.expand_as(target_f)\n",
    "    target_f_cov_eye = \\\n",
    "        torch.mm(target_f_norm, target_f_norm.t()) + torch.eye(3)\n",
    "\n",
    "    source_f_norm_transfer = torch.mm(\n",
    "        _mat_sqrt(target_f_cov_eye),\n",
    "        torch.mm(torch.inverse(_mat_sqrt(source_f_cov_eye)),\n",
    "                 source_f_norm)\n",
    "    )\n",
    "\n",
    "    source_f_transfer = source_f_norm_transfer * \\\n",
    "                        target_f_std.expand_as(source_f_norm) + \\\n",
    "                        target_f_mean.expand_as(source_f_norm)\n",
    "\n",
    "    return source_f_transfer.view(source.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tf = test_transform(args.content_size, args.crop)\n",
    "style_tf = test_transform(args.style_size, args.crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content_path in content_paths:\n",
    "    str_content_path = str(content_path)\n",
    "    splitted_string = str_content_path.split('/')\n",
    "    extended_path = splitted_string[-1].split('.')[-1]\n",
    "    \n",
    "    if do_interpolation:  # one content image, N style image\n",
    "        style = torch.stack([style_tf(Image.open(str(p))) for p in style_paths])\n",
    "        content = content_tf(Image.open(str(content_path))) \\\n",
    "            .unsqueeze(0).expand_as(style)\n",
    "        style = style.to(device)\n",
    "        content = content.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = style_transfer(vgg, decoder, content, style,\n",
    "                                    args.alpha, interpolation_weights)\n",
    "        output = output.cpu()\n",
    "        output_name = output_dir / '{:s}_interpolation{:s}'.format(\n",
    "            content_path.stem, args.save_ext)\n",
    "        save_image(output, str(output_name))\n",
    "\n",
    "    else:  # process one content and one style\n",
    "        for style_path in style_paths:\n",
    "            content = content_tf(Image.open(str(content_path)))\n",
    "            style = style_tf(Image.open(str(style_path)))\n",
    "            if args.preserve_color:\n",
    "                style = coral(style, content)\n",
    "            style = style.to(device).unsqueeze(0)\n",
    "            content = content.to(device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                output = style_transfer(vgg, decoder, content, style,\n",
    "                                        args.alpha)\n",
    "            output = output.cpu()\n",
    "\n",
    "            output_name = output_dir / '{:s}.{:s}'.format(\n",
    "                content_path.stem, extended_path)\n",
    "            save_image(output, str(output_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
